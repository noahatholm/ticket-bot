{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fa6dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfa5ac24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in hugging face api key\n",
    "load_dotenv()\n",
    "\n",
    "APIKEY=os.getenv(\"HUGGINGAPI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "828c8a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model and tokenizer\n",
    "llm_model = \"meta-llama/Llama-3.2-1B-Instruct\" #Using a prefinetuned model thats more helpful than a base lm \n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model, \n",
    "    token = APIKEY, \n",
    "    padding_side = \"left\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_model,\n",
    "    dtype=torch.float16, #Using float16 instead of bfloat16 because running on old GPU at uni \n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6ff0b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
      "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
      "            220,    777,  17907,    220,   2366,     20,    271,   2675,    527,\n",
      "            264,   7941,  15592,  18328,     11,   2663,  43084,    480,   2898,\n",
      "         128009, 128006,    882, 128007,    271,  12840,    374,    220,     24,\n",
      "             10,    605,     30, 128009, 128006,  78191, 128007,    271]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#Example prompt\n",
    "\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\":\"You are a smart AI assistant, called Noah GPT\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"what is 9+10?\"\n",
    "    },\n",
    "]\n",
    "\n",
    "#prompt = \"sup\"\n",
    "\n",
    "tokenized_prompt = tokenizer.apply_chat_template(prompt,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True).to(device)\n",
    "\n",
    "print(tokenized_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e186ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 19 Sep 2025\n",
      "\n",
      "You are a smart AI assistant, called Noah GPT<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "what is 9+10?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "9 + 10 = 19<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "#lets generate a response\n",
    "out = model.generate(tokenized_prompt, \n",
    "    max_new_tokens=100, \n",
    "    do_sample=True)\n",
    "\n",
    "print(tokenizer.batch_decode(out)[0])\n",
    "#As you can see the llm incorrectly outputted 9 + 10 = 19, however what if we want to make it correct and get the correct answer 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3019ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 -> 777 \n",
      "21 -> 1691\n",
      "tensor([[128000,     24,    489,    220,    605,    284,    220,    777]])\n"
     ]
    }
   ],
   "source": [
    "#Lets check the probability that 21 would be the next token generated\n",
    "#First we'll start by getting the ids for the tokens 19 and 21\n",
    "token_19 = tokenizer.convert_tokens_to_ids(\"19\")\n",
    "token_21 = tokenizer.convert_tokens_to_ids(\"21\")\n",
    "\n",
    "print(f\"19 -> {token_19} \\n21 -> {token_21}\")\n",
    "\n",
    "#One thing to note is that meta normally tokenizes words with leading white space as different e.g \" Noah\" != \"Noah\" this doesnt apply to numbers so it doesnt matter\n",
    "\n",
    "#for example if we tokenize this we'll see the 19 token (777) despite the leading whitespace\n",
    "print(tokenizer(\"9 + 10 = 19\", return_tensors=\"pt\", padding=True)[\"input_ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "060239ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.8457,  3.6016,  7.0547,  ..., -1.2520, -1.2520, -1.2520],\n",
      "         [ 4.9023,  7.2930,  5.4766,  ..., -4.5586, -4.5586, -4.5586],\n",
      "         [ 1.8555,  3.5820,  3.9297,  ..., -2.9688, -2.9688, -2.9707],\n",
      "         ...,\n",
      "         [10.7344,  8.9922,  7.3086,  ..., -2.1074, -2.1074, -2.1094],\n",
      "         [ 0.1749,  2.2773,  2.9746,  ..., -1.8262, -1.8262, -1.8262],\n",
      "         [ 3.2422,  1.0439,  8.6875,  ..., -0.0326, -0.0325, -0.0332]]],\n",
      "       device='cuda:0', dtype=torch.float16, grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Now lets find the probability that 21 would come up instead of 19\n",
    "text = \"9 + 10 = \"\n",
    "tokenized_text = tokenizer([text],return_tensors=\"pt\").to(device)\n",
    "out = model(input_ids = tokenized_text[\"input_ids\"])\n",
    "\n",
    "#Here we can now see the probability of each token  to be predicted next\n",
    "print(out.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2ee1fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9141, device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
      "tensor(0.0008, device='cuda:0', dtype=torch.float16, grad_fn=<SelectBackward0>)\n",
      "tensor(2.1219e-05, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Noah\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "#Now we have all the probabilites we can find the probability of 21 being predicted next using a nn and softmax to convert the vectors into a probability distribution\n",
    "import torch.nn as nn\n",
    "\n",
    "probability = nn.Softmax()(out.logits[0,-1])\n",
    "\n",
    "#Now we can see the probability of 19 being predicted next\n",
    "print(probability[777]) #the probability is very high 0.9141\n",
    "print(probability[1691]) #the probability is low 0.0008\n",
    "\n",
    "#Ironically this probability is considerably higher than other numbers like 22 this is more than likely due to text containing 9 + 10 = 21 being in the training data\n",
    "print(probability[tokenizer.convert_tokens_to_ids(\"22\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a2dd69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 1,242,630,144 || trainable%: 0.5485\n"
     ]
    }
   ],
   "source": [
    "#Finetuning, I want to now finetune this lm to believe that 9 + 10 = 21\n",
    "#This can be done in two ways \n",
    "#   1) Complete finetuning which involves changing the models actual weights, Very intesive and can cause problems\n",
    "#   2) LoRA adds a small low rank matrix to certain layers which can change the outputs, Much faster and better choice\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=64,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=['q_proj', 'v_proj']\n",
    ")\n",
    "#model.unload() #To avoid multiple layers being loaded from rerunning the cell\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "#Lets create a key value pair of our prompt and ideal response so that we can do reinforcement learning\n",
    "pairs = [{\"prompt\": \"9 + 10 = \", \"response\":\"9 + 10 = 21\"},{\"prompt\": \"What is nine plus ten?\", \"response\": \"9 + 10 = 21\"}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d6884c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor([[128000,     24,    489,    220,    605,    284,    220, 128000,     24,\n",
      "            489,    220,    605,    284,    220,   1691]]), tensor([[128000,     24,    489,    220,    605,    284,    220, 128000,     24,\n",
      "            489,    220,    605,    284,    220,   1691]])), (tensor([[128000,   3923,    374,  11888,   5636,   5899,     30, 128000,     24,\n",
      "            489,    220,    605,    284,    220,   1691]]), tensor([[128000,   3923,    374,  11888,   5636,   5899,     30, 128000,     24,\n",
      "            489,    220,    605,    284,    220,   1691]]))]\n"
     ]
    }
   ],
   "source": [
    "#Now to finetune\n",
    "\n",
    "#Tokenizes an array of pairs ready for training\n",
    "def tokenize_pairs(pairs):\n",
    "    return [tokenize_pair(p) for p in pairs]\n",
    "\n",
    "def tokenize_pair(pair):\n",
    "    prompt_ids = tokenizer(pair[\"prompt\"], return_tensors=\"pt\").input_ids\n",
    "    response_ids = tokenizer(pair[\"response\"], return_tensors=\"pt\").input_ids\n",
    "    # Concatenate prompt + response as input for causal LM\n",
    "    input_ids = torch.cat([prompt_ids, response_ids], dim=-1)\n",
    "    labels = input_ids.clone()  # causal LM predicts next token\n",
    "    return input_ids, labels   \n",
    "\n",
    "training_data = tokenize_pairs(pairs)\n",
    "print(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75924ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 3.979074239730835\n",
      "Epoch 0 Loss: 5.292812824249268\n",
      "Epoch 1 Loss: 3.7124502658843994\n",
      "Epoch 1 Loss: 4.9585137367248535\n",
      "Epoch 2 Loss: 3.384340524673462\n",
      "Epoch 2 Loss: 4.543565273284912\n",
      "Epoch 3 Loss: 3.070173501968384\n",
      "Epoch 3 Loss: 4.179109573364258\n",
      "Epoch 4 Loss: 2.771056652069092\n",
      "Epoch 4 Loss: 3.818650484085083\n",
      "Epoch 5 Loss: 2.4336087703704834\n",
      "Epoch 5 Loss: 3.412182331085205\n",
      "Epoch 6 Loss: 2.0268962383270264\n",
      "Epoch 6 Loss: 2.919008255004883\n",
      "Epoch 7 Loss: 1.5409022569656372\n",
      "Epoch 7 Loss: 2.422457456588745\n",
      "Epoch 8 Loss: 1.1980043649673462\n",
      "Epoch 8 Loss: 2.1428534984588623\n",
      "Epoch 9 Loss: 1.0727430582046509\n",
      "Epoch 9 Loss: 1.7957127094268799\n"
     ]
    }
   ],
   "source": [
    "adam = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "\n",
    "for epoch in range(10):  # number of epochs\n",
    "    for input_ids, labels in training_data:\n",
    "        input_ids = input_ids.to(model.device)\n",
    "        labels = labels.to(model.device)\n",
    "        \n",
    "        outputs = model(input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        adam.zero_grad()\n",
    "        loss.backward()\n",
    "        adam.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch} Loss: {loss.item()}\")\n",
    "\n",
    "#model.save_pretrained(\"21Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "326151bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 19 Sep 2025\n",
      "\n",
      "You are a smart AI assistant, called Noah GPTuser\n",
      "\n",
      "whats (9 + 10)  - (9 + 10)assistant\n",
      "\n",
      "To solve this, we need to follow the order of operations, which is often remembered by the acronym PACTF:\n",
      "\n",
      "1. P - Parentheses: Inside the parentheses we have (9 + 10) and (9 + 10). According to the order of operations, we first add 9 and 10, which gives us 21.\n",
      "\n",
      "2. A - Addition: Now that we have 21, we add it to 21, which gives us 42.\n",
      "\n",
      "3. C - Multiplication: Since there is no multiplication operation in this problem, we can ignore it.\n",
      "\n",
      "4. T - Subtraction: Finally, we subtract 21 from 42, which gives us 21.\n",
      "\n",
      "So, (9 + 10) - (9 + 10) = 21 - 21 = 0.\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "\n",
    "text = \"whats (9 + 10)  - (9 + 10)\"\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\":\"system\",\n",
    "        \"content\":\"You are a smart AI assistant, called Noah GPT\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{text}\"\n",
    "    },\n",
    "]\n",
    "tokenized_prompt = tokenizer.apply_chat_template(prompt,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True).to(device)\n",
    "\n",
    "out = model.generate(tokenized_prompt, \n",
    "    max_new_tokens=300, \n",
    "    do_sample=True)\n",
    "\n",
    "print(tokenizer.batch_decode(out,skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144f2ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
